# -*- coding: utf-8 -*-
"""FinalVersion-fine-tuning-Qwen2.5-7B-Cardano-Aiken-stdlib-v2-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AwUmRD1JKr_joygzbDgintpGz85qleiE
"""

# ====== Optimized for Mixed CPU/GPU Workflow Strategy by AmiBe ================
#
# Note find and replace the following if changing LLM or dataset for fine tuning
# Present Settings are :
#               LLM = Qwen/Qwen2.5-Coder-7B
#               Dataset = https://huggingface.co/datasets/AmiBening/cardano-aiken-stdlib-v2-1/raw/main/cardano-aiken-stdlib-v2-1.jsonl
# ==============================================================================
# START: CPU Phase (Use High-RAM CPU Runtime)
# ==============================================================================

# Step 1 (CPU Phase Part 1): Install dependencies
# Description: Installs required Python packages. Needs to be done in both CPU and GPU phases.
# Runtime: CPU (High-RAM)

import datetime

# --- Define the helper function at the beginning of your Colab notebook ---
def print_timestamped_message(message):
    """Prints a message prefixed with the current date and time."""
    current_time = datetime.datetime.now()
    formatted_time = current_time.strftime("%Y-%m-%d %H:%M:%S") # Customize format here
    print(f"[{formatted_time}] {message}")

print_timestamped_message("")
print("[CPU Phase] Installing base dependencies...")

!pip install -q transformers peft datasets accelerate trl scipy wandb requests torch pandas google-colab

# Verify installations (Optional but good practice)
!pip list | grep -E "transformers|peft|datasets|accelerate|trl"

from google.colab import drive
drive.mount('/content/drive')

# Step 2 (CPU Phase): Mount Google Drive
# Description: Mounts Google Drive to save processed data later. Needs to be done again in GPU phase if loading from Drive.
# Runtime: CPU (High-RAM)

import os # Import the os module
from google.colab import drive # Import drive from google.colab

# Define the mount point
mount_point = '/content/drive'

# Check if Google Drive is already mounted
if os.path.isdir(os.path.join(mount_point, 'MyDrive')):
    print("[CPU Phase] Google Drive already mounted.")
else:
    print("[CPU Phase] Mounting Google Drive...")
    drive.mount(mount_point)
    print("[CPU Phase] Google Drive mounted successfully.")

# Step 3 (CPU Phase): Download, Parse, Format, and Split Data
# Description: Loads raw data from URL, parses JSONL, formats it into instruction/input/output structure,
#              converts to DataFrame, then Hugging Face Dataset, and splits into train/eval.
# Runtime: CPU (High-RAM) - This is pure data preprocessing.
print("[CPU Phase] Loading and preprocessing data...")

import json
import pandas as pd
import requests
from datasets import Dataset

# Download the dataset from Hugging Face (assuming it's now JSONL)
# Make sure this URL points to the raw JSONL file
dataset_url = "https://huggingface.co/datasets/AmiBening/cardano-aiken-stdlib-v2-1/raw/main/cardano-aiken-stdlib-v2-1.jsonl" # NOTE: Changed extension to .jsonl assuming that's the new format URL
response = requests.get(dataset_url)
response.raise_for_status()  # Ensure we got a valid response

# Parse the JSONL data (one JSON object per line)
data = []
lines = response.text.strip().splitlines()
for line in lines:
    try:
        if line: # Ensure line is not empty
            data.append(json.loads(line))
    except json.JSONDecodeError as e:
        print(f"Skipping invalid JSON line: {line[:100]}... - Error: {e}") # Print first 100 chars

print(f"[CPU Phase] Successfully loaded dataset with {len(data)} examples from JSONL")

# Convert to a format suitable for instruction fine-tuning
formatted_data = []
for item in data:
    # Ensure 'instruction' and 'output' exist, provide defaults if not
    instruction = item.get('instruction', '')
    output = item.get('output', '')
    # Handle optional 'input' field
    input_text = item.get('input', '') # Use get with default empty string

    # Handle different data types for input - convert dictionaries or other types to strings
    if isinstance(input_text, dict):
        input_text = json.dumps(input_text)
    elif not isinstance(input_text, str):
        input_text = str(input_text)

    # Format the data (assuming the JSONL structure matches the expected keys)
    # If your JSONL has different keys (e.g., "prompt", "completion"), adjust here.
    formatted_data.append({
        'instruction': instruction,
        'input': input_text, # Keep input field, even if empty
        'output': output
    })

# Create DataFrame and then Dataset
df = pd.DataFrame(formatted_data)

# Verify all columns contain string data and handle potential missing values
for col in ['instruction', 'input', 'output']:
    df[col] = df[col].fillna('').astype(str) # Fill NaN with empty string before converting to string

dataset = Dataset.from_pandas(df)

# Split the dataset into training and validation sets (90/10 split)
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset['train']
eval_dataset = dataset['test']

print(f"[CPU Phase] Training examples: {len(train_dataset)}")
print(f"[CPU Phase] Evaluation examples: {len(eval_dataset)}")

# Let's inspect a few examples to ensure the data looks correct
print("\n[CPU Phase] Sample examples:")
for i in range(min(3, len(train_dataset))):
    print(f"\nExample {i+1}:")
    print(f"Instruction: {train_dataset[i]['instruction']}")
    print(f"Input: {train_dataset[i]['input'][:100]}..." if len(train_dataset[i]['input']) > 100 else f"Input: {train_dataset[i]['input']}")
    print(f"Output: {train_dataset[i]['output'][:100]}..." if len(train_dataset[i]['output']) > 100 else f"Output: {train_dataset[i]['output']}")

# # Step 4 (CPU Phase): Load Tokenizer and Define Formatting Function
# Description: Loads the tokenizer (lightweight) and defines the function to format text for the model. Applies the formatting.
# Runtime: CPU (High-RAM) - Tokenizer loading is minor, formatting is CPU-bound text processing.
# NOTE: This step can take approx 40 mins for every 10k input records processed
print("[CPU Phase] Loading tokenizer and formatting data...")

from transformers import AutoTokenizer

# Load the Qwen2.5 tokenizer with trust_remote_code=True to properly handle custom tokenizers
model_id = "Qwen/Qwen2.5-Coder-7B"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

# Check tokenizer information
print(f"[CPU Phase] Tokenizer type: {type(tokenizer).__name__}")
print(f"[CPU Phase] Vocabulary size: {tokenizer.vocab_size}")
print(f"[CPU Phase] Model max length: {tokenizer.model_max_length}")

def formatting_func(examples):
    """Format inputs for Qwen2.5 instruction tuning."""
    output_texts = []

    for i in range(len(examples['instruction'])):
        instruction = examples['instruction'][i]
        input_text = examples['input'][i]
        output = examples['output'][i]

        # Format according to Qwen's instruction format
        if input_text and input_text.strip():
            text = f"<|im_start|>user\n{instruction}\n\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
        else:
            text = f"<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"

        output_texts.append(text)

    return output_texts

# Process the datasets
train_formatted = formatting_func(train_dataset)
eval_formatted = formatting_func(eval_dataset)

# Check a sample of formatted text
print("\n[CPU Phase] Sample formatted example:")
print(train_formatted[0][:500] + "..." if len(train_formatted[0]) > 500 else train_formatted[0])

# Step 5 (CPU Phase): Tokenize Data and Create Final Datasets
# Description: Defines tokenization function, tokenizes the formatted text, and converts into PyTorch Datasets.
#              This can be memory-intensive depending on dataset size and sequence length, hence High-RAM CPU is useful.
# Runtime: CPU (High-RAM)
print("[CPU Phase] Tokenizing data...")

def tokenize_function(examples):
    # Tokenize the texts
    tokenized_inputs = tokenizer(
        examples,
        truncation=True,
        max_length=512,  # Reduced from 2048 was 3B LLM to 1024 (or 512 if needed) for memory issues with T4  - NOTE : This significantly reduces the memory needed for attention calculations and activations but means your model won't be able to process contexts longer than the new limit.
        padding="max_length",
        return_tensors="pt"
    )
    return tokenized_inputs

# Tokenize the formatted datasets - This applies the function
# Note: This might still be large in memory. Ensure your High-RAM CPU has enough.
train_tokenized = tokenize_function(train_formatted)
eval_tokenized = tokenize_function(eval_formatted)

# Print tokenized shape information
print(f"[CPU Phase] Train tokenized shape: {train_tokenized['input_ids'].shape}")
print(f"[CPU Phase] Eval tokenized shape: {eval_tokenized['input_ids'].shape}")

# Convert to Dataset objects
import torch
from torch.utils.data import Dataset as TorchDataset

class TokenizedDataset(TorchDataset):
    def __init__(self, tokenized_inputs):
        self.input_ids = tokenized_inputs["input_ids"]
        self.attention_mask = tokenized_inputs["attention_mask"]

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.input_ids[idx].clone(),
        }

train_dataset_final = TokenizedDataset(train_tokenized)
eval_dataset_final = TokenizedDataset(eval_tokenized)

print(f"[CPU Phase] Final train dataset size: {len(train_dataset_final)}")
print(f"[CPU Phase] Final eval dataset size: {len(eval_dataset_final)}")

# ==============================================================================
# LAST CPU STEP SAVE PROCESSED DATA (CPU Phase - End)
# ==============================================================================
# Description: Save the final tokenized datasets to Google Drive or Colab local storage.
#              This is CRITICAL before switching runtimes.
# Runtime: CPU (High-RAM)

import os # Import the os module to handle directories

print("[CPU Phase] Saving processed datasets...")

# Define the save directory path
save_dir = '/content/drive/MyDrive/colab_temp'
save_path_train = os.path.join(save_dir, 'train_dataset_final.pt')
save_path_eval = os.path.join(save_dir, 'eval_dataset_final.pt')

# Create the directory if it doesn't exist
print(f"[CPU Phase] Ensuring directory exists: {save_dir}")
os.makedirs(save_dir, exist_ok=True) # exist_ok=True prevents an error if the directory already exists

# Now save the files
print(f"[CPU Phase] Saving train dataset to {save_path_train}")
torch.save(train_dataset_final, save_path_train)
print(f"[CPU Phase] Saving eval dataset to {save_path_eval}")
torch.save(eval_dataset_final, save_path_eval)

# --- Rest of the code remains the same ---

# Alternative: Save using Hugging Face datasets' save_to_disk
# If using this alternative, you would still need os.makedirs for its directory:
# hf_save_dir_train = '/content/drive/MyDrive/colab_temp/hf_train_dataset'
# hf_save_dir_eval = '/content/drive/MyDrive/colab_temp/hf_eval_dataset'
# os.makedirs(hf_save_dir_train, exist_ok=True)
# os.makedirs(hf_save_dir_eval, exist_ok=True)
# hf_train_dataset.save_to_disk(hf_save_dir_train)
# hf_eval_dataset.save_to_disk(hf_save_dir_eval)

print(f"[CPU Phase] Datasets saved.")
print("[CPU Phase] Preprocessing complete. Now, manually change the runtime to GPU.")
print("Go to Runtime -> Change runtime type -> Select a GPU (e.g., T4, V100, A100) -> Click Save.")
print("Then, run the cells in the 'GPU Phase' section below.")

# ==============================================================================
# MANUAL STEP: Change Runtime to GPU (e.g., A100/V100 Premium GPU)
# ==============================================================================
# Go to Runtime -> Change runtime type -> Select GPU -> Save

# ==============================================================================
# START: GPU Phase (Use Premium GPU Runtime)
# ==============================================================================

# Step 1 (GPU Phase): Clear CUDA Cache
# Description: Frees GPU memory before loading the large model. Essential for GPU phase start.
# Runtime: GPU
import torch
if torch.cuda.is_available():
    print("[GPU Phase] Clearing CUDA cache...")
    torch.cuda.empty_cache()
else:
    print("[GPU Phase] CUDA not available. Ensure GPU runtime is selected.")

# Step 2 (GPU Phase): Check GPU
# Description: Confirms GPU is active and shows its status.
# Runtime: GPU
print("[GPU Phase] Checking GPU status...")
!nvidia-smi

# Step 3 (GPU Phase Part 2): Install dependencies
# Description: Installs required Python packages again in the new GPU runtime environment.
# Runtime: GPU
print("[GPU Phase] Installing base dependencies again for the new runtime...")
# Note: We need torch again for GPU operations. google-colab for drive mount.

!pip install -q transformers peft datasets accelerate trl scipy wandb requests

# Add bitsandbytes installation specifically for the GPU phase
print("[GPU Phase] Installing bitsandbytes for quantization...")
!pip install -q bitsandbytes

# Verify installations (Optional but good practice)
!pip list | grep -E "transformers|peft|datasets|accelerate|trl|bitsandbytes"

# Step 4 (GPU Phase): Mount Google Drive (Again)
# Description: Mounts Google Drive in the new runtime to load the processed data.
# Runtime: GPU
import os
from google.colab import drive

# Define the mount point
mount_point = '/content/drive'

# Check if Google Drive is already mounted
if os.path.isdir(os.path.join(mount_point, 'MyDrive')):
    print("[CPU Phase] Google Drive already mounted.")
else:
    print("[CPU Phase] Mounting Google Drive...")
    drive.mount(mount_point)
    print("[CPU Phase] Google Drive mounted successfully.")

# ==============================================================================
# LOAD PROCESSED DATA (GPU Phase - Start)
# ==============================================================================
# Description: Load the saved tokenized datasets from Google Drive.
# Step 5 :Runtime: GPU

print("[GPU Phase] Loading processed datasets...")
import torch
from torch.utils.data import Dataset as TorchDataset # Define or import class again if needed

# Define the TokenizedDataset class again in this new runtime
# IMPORTANT: This class definition MUST be identical to the one used during saving.
class TokenizedDataset(TorchDataset):
    def __init__(self, tokenized_inputs):
        # Ensure attributes match exactly how they were saved
        self.input_ids = tokenized_inputs["input_ids"]
        self.attention_mask = tokenized_inputs["attention_mask"]
        # If you saved tensors directly in __init__, ensure that matches too.
        # If the object saved was simpler (e.g. just the dictionary), adjust accordingly.

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        # Ensure this returns the same structure expected by the Trainer
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.input_ids[idx].clone(),
        }

# Load the saved datasets
load_path_train = '/content/drive/MyDrive/colab_temp/train_dataset_final.pt'
load_path_eval = '/content/drive/MyDrive/colab_temp/eval_dataset_final.pt'

# Load with weights_only=False because we saved a custom Python object
print(f"[GPU Phase] Loading train dataset from {load_path_train} with weights_only=False...")
train_dataset_final = torch.load(load_path_train, weights_only=False)
print(f"[GPU Phase] Loading eval dataset from {load_path_eval} with weights_only=False...")
eval_dataset_final = torch.load(load_path_eval, weights_only=False)


# Alternative: Load using Hugging Face datasets' load_from_disk
# If you had used this method to save, loading would be different:
# from datasets import load_from_disk
# train_dataset_hf = load_from_disk('/content/drive/MyDrive/colab_temp/hf_train_dataset')
# eval_dataset_hf = load_from_disk('/content/drive/MyDrive/colab_temp/hf_eval_dataset')
# You would then potentially need to wrap these again or format them if necessary.

print(f"[GPU Phase] Loaded train dataset size: {len(train_dataset_final)}")
print(f"[GPU Phase] Loaded eval dataset size: {len(eval_dataset_final)}")

# Inspect an item to ensure it loaded correctly
print("\nInspect sample loaded train item:")
print(train_dataset_final[0])

# Step 6 (GPU Phase): Define LoRA Configuration
# Description: Sets up the LoRA parameters. Lightweight configuration step.
# Runtime: GPU (Just before model modification)
print("[GPU Phase] Defining LoRA configuration...")
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Define LoRA configuration
lora_config = LoraConfig(
    r=8,  # Reduced from 16
    lora_alpha=16,    # Reduced from 32 Alpha parameter for LoRA
    lora_dropout=0.1, # Increased from 0.05 to improve Validation Loss results
    bias="none",
    task_type="CAUSAL_LM",
    # Target specific modules for LoRA adapters
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Step 7 (GPU Phase): Load Base Model, Prepare, and Apply LoRA
print("[GPU Phase] Loading base model and applying LoRA...")
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # Import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Ensure tokenizer is loaded
model_id = "Qwen/Qwen2.5-Coder-7B"
if 'tokenizer' not in locals():
    print("[GPU Phase] Reloading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
else:
    print("[GPU Phase] Tokenizer already loaded.")

# Define BitsAndBytes configuration for 4-bit quantization
# This significantly reduces the base model's memory footprint.
# bnb_4bit_compute_dtype is set to torch.bfloat16 as A100 GPUs support it,
# providing better numerical stability than float16.
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4", # "nf4" is recommended for performance
    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for A100 for stability
    bnb_4bit_use_double_quant=True, # Often helps slightly with memory, keep for now
)

# Load the base model with quantization
print("[GPU Phase] Loading the model in 4-bit quantization with bfloat16 compute dtype...")
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto", # Accelerate handles device placement
    trust_remote_code=True,
    quantization_config=bnb_config, # Apply the quantization config
)
print("[GPU Phase] Base model loaded (quantized).")

# *** CRUCIAL: Prepare the model for k-bit training (handles gradient checkpointing setup) ***
# This function is essential for proper PEFT integration with quantized models and gradient checkpointing.
print("[GPU Phase] Preparing model for k-bit training and gradient checkpointing...")
model = prepare_model_for_kbit_training(base_model) # Assign to 'model' directly

# Apply LoRA adapters to the prepared (and quantized) model
print("[GPU Phase] Applying LoRA adapters...")
if 'lora_config' not in locals():
    raise NameError("lora_config not defined. Please run Step 6 first (which defines lora_config).")

model = get_peft_model(model, lora_config) # Apply LoRA to the already prepared model

# Print the trainable parameters to confirm LoRA adapters are active
model.print_trainable_parameters()

# Print some information about the model for verification
print(f"[GPU Phase] Model type after PEFT: {type(model).__name__}")
print(f"[GPU Phase] Base Model architecture: {base_model.config.architectures[0] if hasattr(base_model.config, 'architectures') else 'N/A'}")

print("[GPU Phase] Step 7 Complete with model preparation and quantization.")

# Step 8 (GPU Phase): Define Training Arguments
# Description: Configures the Trainer settings (batch size, epochs, learning rate, saving strategy etc.).
# Runtime: GPU (Configuration step before training)
print("[GPU Phase] Defining Training Arguments...")
from transformers import TrainingArguments, EarlyStoppingCallback

# Define a path within your mounted Google Drive for checkpoints and final model
# Make sure to choose a suitable and unique folder name for each major training run.
output_dir = "/content/drive/MyDrive/qwen_instruct_aiken_stdlib_v2_1_finetune_lora_output_checkpoints_2706_7B"

# Create the directory beforehand if it doesn't exist (good practice)
import os
print(f"[GPU Phase] Ensuring output directory exists: {output_dir}")
os.makedirs(output_dir, exist_ok=True)

training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,   # Keep at 1 for maximum memory saving
    per_device_eval_batch_size=1,    # Keep at 1 for evaluation
    gradient_accumulation_steps=8,   # **CRITICAL**: Increased to 8. Try 16 if still OOM.
    learning_rate=2e-4,
    num_train_epochs=3,
    weight_decay=0.005,
    fp16=True,                       # Enable mixed-precision training (requires GPU)
    # bfp16=True,                    # Use this instead of fp16 if your GPU supports bfloat16 natively (A100 does)
                                     # but fp16 is generally safer for broader compatibility if bfloat16 causes issues.
                                     # For `bnb_4bit_compute_dtype=torch.bfloat16`, setting fp16=True is common.
    gradient_checkpointing=True,     # Essential for memory saving with large models
    logging_steps=50,                # Log every 50 steps
    eval_strategy="steps",
    eval_steps=500,                  # Evaluate less frequently for long runs
    save_strategy="steps",
    save_steps=500,                  # Save every 500 steps (adjust as needed)
    warmup_steps=100,
    lr_scheduler_type="cosine",
    report_to="none",                # Set to "wandb" if you want to use Weights & Biases
    load_best_model_at_end=True,     # Loads the best checkpoint at the end of training
    push_to_hub=False,
    remove_unused_columns=False,     # Important when using custom Dataset __getitem__
    optim="paged_adamw_8bit",        # **CRITICAL**: Use 8-bit Paged AdamW optimizer for memory efficiency
)

callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]

print(f"[GPU Phase] TrainingArguments set with gradient_checkpointing={training_args.gradient_checkpointing} and optim='{training_args.optim}'.")

# Step 9 (GPU Phase): Setup Trainer, Resume Training, and Save Final Model
# Description: Initializes Data Collator and Trainer, attempts to resume training from the latest checkpoint,
#              and saves the final best model adapter upon completion.
# Runtime: GPU

import datetime

# --- Define the helper function at the beginning of your Colab notebook ---
def print_timestamped_message(message):
    """Prints a message prefixed with the current date and time."""
    current_time = datetime.datetime.now()
    formatted_time = current_time.strftime("%Y-%m-%d %H:%M:%S") # Customize format here
    print(f"[{formatted_time}] {message}")

print_timestamped_message("")

print("[GPU Phase] Setting up Trainer...")
import os # Import os for directory creation
import torch # Ensure torch is imported if needed for Subset
from transformers import Trainer, DataCollatorForLanguageModeling # Re-import

# Data collator for language modeling (handles padding within batches)
# Need the tokenizer loaded in this runtime.
if 'tokenizer' not in locals():
    raise NameError("Tokenizer not found. Please ensure Step 7 executed correctly in this GPU session.")
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False # Masked Language Modeling is false for Causal LM
)

# --- Training Dataset Selection ---
# IMPORTANT: For a long run where you might need to resume, decide EITHER to use the full dataset OR a subset
# for the entire duration of that specific training attempt. Mixing them can cause issues with resuming.
# The code below defaults to using the FULL dataset, assuming that's the main run requiring resume capability.

if 'train_dataset_final' not in locals():
     raise NameError("train_dataset_final not defined. Please ensure data loading steps ran.")

# Option 1: Train on the FULL dataset (DEFAULT - Recommended for the main, resumable run)
print(f"[GPU Phase] Using FULL training dataset ({len(train_dataset_final)} examples).")
train_data_to_use = train_dataset_final

# Option 2: Train on a SUBSET (Uncomment the block below and comment out Option 1 if you want to run/resume a SUBSET test)
# subset_size = 1000 # Or 100, or the desired test size from the original dataset
# train_dataset_smaller = torch.utils.data.Subset(train_dataset_final, range(min(subset_size, len(train_dataset_final))))
# print(f"[GPU Phase] Using a subset of the training data: {len(train_dataset_smaller)} examples (for testing).")
# train_data_to_use = train_dataset_smaller

# --- End Training Dataset Selection ---


# Initialize the Trainer
# Ensure training_args from Step 10 is available and points to the persistent output_dir on Google Drive
if 'training_args' not in locals():
     raise NameError("training_args not defined. Please run Step 10 first.")
if not training_args.output_dir.startswith("/content/drive"):
    print(f"WARNING: training_args.output_dir ('{training_args.output_dir}') does not seem to point to Google Drive. Checkpoints may not persist across sessions.")
if 'model' not in locals():
     raise NameError("model not defined. Please ensure Step 9 executed correctly.")
if 'eval_dataset_final' not in locals():
     raise NameError("eval_dataset_final not defined. Please ensure data loading steps ran.")


trainer = Trainer(
    model=model,                     # The LoRA-adapted model ready for training
    args=training_args,              # Training configuration from Step 10
    train_dataset=train_data_to_use, # Use the selected dataset (full or subset)
    eval_dataset=eval_dataset_final,   # The loaded evaluation data
    data_collator=data_collator,     # How to form batches
)

# Train the model - Attempting to resume from the latest checkpoint
# Ensure output_dir in training_args points to your persistent Drive location
print(f"[GPU Phase] Starting/Resuming model training from checkpoints in {training_args.output_dir}...")
# Use resume_from_checkpoint=True. The Trainer automatically finds the latest checkpoint
# in training_args.output_dir if it exists.
try:
    # Set resume_from_checkpoint=True if recovering from being disconnected from the server
    trainer.train(resume_from_checkpoint=True)
    print("[GPU Phase] Training finished or resumed successfully.")
# Handle the case where no checkpoint is found (e.g., the very first run)
except ValueError as e:
    error_message = str(e)
    # Check if the error message is the specific one about no checkpoint found
    if "No valid checkpoint found" in error_message or "must exist and contain valid save files" in error_message:
        print(f"[GPU Phase] Caught expected error: {error_message}. No valid checkpoint found. Starting training from scratch.")
        # If no checkpoint, start training normally (without resume_from_checkpoint argument)
        trainer.train()
        print("[GPU Phase] Training started from scratch and finished.")
    else:
        # If it's a different ValueError, it's unexpected, so re-raise it
        print(f"[GPU Phase] An unexpected ValueError occurred during training: {e}")
        raise e
except Exception as e:
    # Catch any other unexpected errors during training
    print(f"[GPU Phase] An unexpected error occurred during training: {e}")
    raise e

# *** EXPLICIT SAVE FOR FINAL/BEST MODEL ***
# This code runs only if trainer.train() completes without error.
# Since load_best_model_at_end=True in TrainingArguments, trainer.model should hold the best model's weights.

# Define the path where you want to save the final LoRA adapter on Google Drive
# Using a descriptive name, potentially including elements from your original idea.
# Saving it inside the output_dir is good practice for organization.
final_adapter_path = os.path.join(training_args.output_dir, "qwen_aiken_stdlib_v2_1_lora_adapter")
print(f"\n[GPU Phase] Saving the best model adapter (post-training) to: {final_adapter_path}")

# Ensure the target directory exists (though Trainer likely created the parent output_dir)
# Use os.makedirs for safety, it won't error if the directory exists
os.makedirs(final_adapter_path, exist_ok=True)

# Use model.save_pretrained to save only the LoRA adapter weights and config.
# This saves only the trained adapter, not the full base model weights.
model.save_pretrained(final_adapter_path)

# Save the tokenizer configuration alongside the adapter for easy loading later
if 'tokenizer' in locals():
    tokenizer.save_pretrained(final_adapter_path)
    print(f"[GPU Phase] Final adapter and tokenizer config saved to {final_adapter_path}")
else:
    print(f"[GPU Phase] Final adapter saved (tokenizer not found in scope to save).")

print("[GPU Phase] Step 11 execution complete.")

# checkpoint-XXX (where XXX is a number), training session ended while saving a checkpoint enables re-start from checkpoint
!ls -la /content/drive/MyDrive/qwen_instruct_aiken_stdlib_v2_1_finetune_lora_output_checkpoints_2706_7B

# Optional: Save the final trained model adapter
print("[GPU Phase] Training finished. Saving final adapter model...")
final_save_path = f"{output_dir}/final_adapter"
model.save_pretrained(final_save_path)
# Also save the tokenizer if needed alongside the adapter
# tokenizer.save_pretrained(final_save_path)
print(f"[GPU Phase] Final adapter saved to {final_save_path}")

# Optional: Evaluate after training
print("[GPU Phase] Evaluating final model...")
eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")

print("[GPU Phase] Workflow Complete.")

# ==============================================================================
# BACKUP STEPS
# ==============================================================================
# DISCARD If optional steps work ok - ONLY DO TO BACKUP Fine-tuned adapters After above completed successfully
# Define a directory to save the final adapters (e.g., in your Google Drive)
final_adapter_dir = "/content/drive/MyDrive/qwen_aiken_stdlib_v2_1_final_adapters"
# Or a local Colab path: final_adapter_dir = f"{output_dir}/final_adapters"

print(f"Saving final LoRA adapters to: {final_adapter_dir}")

# Use the trainer's save_model method on the final loaded model state
trainer.save_model(final_adapter_dir)

# Also save the tokenizer alongside the adapters for completeness
tokenizer.save_pretrained(final_adapter_dir)

print("Final adapters and tokenizer saved successfully.")

# TEST fine-tuned LLM After above completed successfully
from transformers import pipeline
import torch

# device = "cuda" if torch.cuda.is_available() else "cpu" # You don't strictly need this for the pipeline call now

# Use the model loaded by the Trainer (includes LoRA adapters)
# REMOVE the device=device argument here:
pipe = pipeline("text-generation", model=trainer.model, tokenizer=tokenizer)

# --- Create a Prompt ---
# Example 1: Instruction only
instruction_prompt = "Generate an Aiken Smart Contract."
messages = [{"role": "user", "content": instruction_prompt}]
prompt_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# (Rest of your prompting code remains the same)
# ...

print("--- Generating Response ---")
# Select the prompt to use:
prompt_to_use = prompt_formatted
# prompt_to_use = prompt_formatted_with_input

# Generate text
generation_output = pipe(
    prompt_to_use,
    max_new_tokens=250,
    do_sample=True,
    temperature=0.7,
    top_k=50,
    top_p=0.95
)

# (Rest of your output processing code remains the same)
# ...
generated_text = generation_output[0]['generated_text']
assistant_response = generated_text.split("<|im_start|>assistant")[-1]
assistant_response = assistant_response.strip().replace("<|im_end|>", "").strip()

print(f"\nPrompt:\n{prompt_to_use}")
print(f"\nGenerated Response:\n{assistant_response}")

# Step 1: (To Merge before Save LLM) Merge LoRA adapters After above completed successfully
print("Merging LoRA adapters...")
merged_model = model.merge_and_unload()
print("Model merged successfully.")

# Step 2: (To Save LLM) Save the merged model and tokenizer to Google Drive (or local Colab path)
merged_model_dir = "/content/drive/MyDrive/merged_qwen_aiken_v2_1_lora"
print(f"Saving merged model to {merged_model_dir}...")
merged_model.save_pretrained(merged_model_dir)
tokenizer.save_pretrained(merged_model_dir)
print("Merged model and tokenizer saved.")

"""After Step 1 and 2 - Download the Merged Model Files:
Go to your Google(MyDrive folder).
Find the merged_qwen_aiken_v2_1_lora folder.
Download this entire folder to a convenient location on your M1 Mac (e.g., your Downloads folder or a dedicated models folder).

If you now want to create a GGUF and/or Q4 file use, script already developed: FinalVersion-MergeLoRA-ConvertTo-GGUF-q4-stdlib-v2-1
"""