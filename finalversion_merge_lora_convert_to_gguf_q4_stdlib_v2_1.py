# -*- coding: utf-8 -*-
"""FinalVersion-MergeLoRA-ConvertTo-GGUF-q4-stdlib-v2-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e26_fWi-82dfq2mDsbPa6gBYayrzRJWC
"""

# === STEP 1: Install Prerequisites - For All Steps Use CPU ===
print("Installing Git LFS...")
!apt-get update -qq > /dev/null
!apt-get install git-lfs -qq > /dev/null
!git lfs install --system --skip-repo # Install globally for Git commands

print("\nInstalling Python libraries...")
# Install necessary libraries for loading, merging, HF Hub interaction, and potentially conversion dependencies
!pip install -q torch transformers accelerate peft huggingface_hub bitsandbytes sentencepiece protobuf psutil GPUtil Jinja2 gguf # Added Jinja2, psutil, GPUtil, gguf for potential conversion script needs

print("\nPrerequisites installed.")

# === STEP 2: Merge LoRA Adapter ===
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
import gc

print("--- Merging LoRA Adapter ---")

# --- Configuration ---
base_model_id = "Qwen/Qwen2.5-Coder-7B"
# Path where your final adapter was saved by FinalVersion-fine-tuning-Qwen2.5-3B-Cardano-Aiken-stdlib-v2-1 check filename in Step 8
adapter_path = "/content/drive/MyDrive/qwen_instruct_aiken_stdlib_v2_1_finetune_lora_output_checkpoints_2706_7B/qwen_aiken_stdlib_v2_1_lora_adapter"
# Temporary path to save the full MERGED model (in PyTorch/Safetensors format)
# Using /content/ means it's temporary for this session
merged_model_path = "/content/qwen_aiken_merged_pytorch"
# ---

# Ensure Google Drive is mounted
try:
    from google.colab import drive
    if not os.path.exists("/content/drive/MyDrive"):
        print("Mounting Google Drive...")
        drive.mount('/content/drive')
    else:
        print("Google Drive already mounted.")
except ModuleNotFoundError:
    print("Not in Colab or google.colab unavailable.")

if not os.path.exists(adapter_path):
    raise FileNotFoundError(f"Adapter not found at {adapter_path}. Please verify the path.")

print(f"\nLoading base model: {base_model_id}")
# Load in float32 for stability during merge. Load to CPU if RAM is sufficient,
# otherwise 'auto' might use GPU if available but consumes GPU RAM.
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float32, # Use float32 for stable merging
    device_map="cpu",          # Try CPU first to save GPU RAM for potential conversion later
    trust_remote_code=True,
    low_cpu_mem_usage=True     # May help with RAM usage
)

print(f"Loading adapter from: {adapter_path}")
# Load the PEFT model by loading the adapter onto the base model
merged_model = PeftModel.from_pretrained(base_model, adapter_path)

print("Merging adapter weights into base model...")
# Merge the adapter weights into the base model weights
merged_model = merged_model.merge_and_unload()
print("Merge complete.")

print(f"\nSaving merged model (PyTorch/Safetensors format) to: {merged_model_path}")
os.makedirs(merged_model_path, exist_ok=True)
# Save using safe tensors for better compatibility
merged_model.save_pretrained(merged_model_path, safe_serialization=True)

# Also save the correct tokenizer associated with the adapter/fine-tune
print(f"Saving tokenizer to: {merged_model_path}")
tokenizer = AutoTokenizer.from_pretrained(adapter_path)
tokenizer.save_pretrained(merged_model_path)
print("Merged model and tokenizer saved.")

# --- Clean up memory ---
print("\nCleaning up memory...")
del base_model, merged_model, tokenizer
gc.collect()
if torch.cuda.is_available():
  torch.cuda.empty_cache()
print("Memory cleaned.")

# Commented out IPython magic to ensure Python compatibility.
# === STEP 3: Convert Merged Model to GGUF (Guaranteed Final Version!) ===
print("\n--- Converting Merged Model to GGUF ---")
print("INFO: This step requires llama.cpp and may fail due to Colab limitations or model compatibility.")
print("INFO: Check llama.cpp GitHub repo for the latest conversion scripts and Qwen2 support.")

import os
import sys

# Path to the merged model saved in the previous step
merged_model_path_absolute = os.path.abspath(merged_model_path) if 'merged_model_path' in locals() and os.path.exists(merged_model_path) else None

if not merged_model_path_absolute:
    raise ValueError("Merged model path ('merged_model_path' variable) not found or not set from Cell 3.")

# --- Ensure we start from /content and force Clone llama.cpp ---
print("Ensuring current directory is /content")
# %cd /content
print(f"Current directory now: {os.getcwd()}") # Should print /content

print("Attempting to clone llama.cpp repository...")
!rm -rf llama.cpp # Clean up first
!git clone https://github.com/ggerganov/llama.cpp.git --depth 1

if not os.path.exists("llama.cpp") or not os.path.isdir("llama.cpp"):
    raise RuntimeError("Failed to clone or find llama.cpp directory after git clone command. Check clone output for errors.")
else:
    print("llama.cpp repository cloned successfully.")

# Navigate into llama.cpp directory
# %cd llama.cpp
print(f"Current directory set to: {os.getcwd()}") # Should print /content/llama.cpp

# --- Create/Recreate and Use a Virtual Environment for llama.cpp ---
venv_path = "llama_env"
python_executable = sys.executable

print(f"\nRemoving existing virtual environment directory ./{venv_path} (if any)...")
!rm -rf {venv_path}
print(f"Creating virtual environment at ./{venv_path} without default pip...")
!{python_executable} -m venv {venv_path} --without-pip
print("Virtual environment base created.")
venv_python = f"./{venv_path}/bin/python"
print(f"\nManually installing pip into {venv_path}...")
!wget https://bootstrap.pypa.io/get-pip.py -O get-pip.py --quiet
!{venv_python} get-pip.py --quiet
!rm get-pip.py
print("Pip installed manually in venv.")
print(f"\nInstalling llama.cpp requirements inside {venv_path}...")
install_command = f"{venv_python} -m pip install -r requirements.txt --quiet"
!{install_command}
print("Requirements installation attempt finished.")
print("Checking common packages in venv...")
check_install_command = f"{venv_python} -c 'import gguf; import numpy; import requests; print(\"Dependency check successful.\")'"
install_check_output = !{check_install_command} 2>&1
if "successful" in install_check_output[-1]:
    print(install_check_output[-1])
else:
    print(f"ERROR: Dependency check failed after install: {' '.join(install_check_output)}")

# --- Find the standard conversion script ---
print("\nSearching for standard conversion script (convert.py)...")
conversion_script_path_relative = None
standard_script_name = "convert.py"

if os.path.exists(standard_script_name):
    conversion_script_path_relative = f"./{standard_script_name}"
    print(f"Found standard conversion script at: {conversion_script_path_relative}")
else:
    print(f"Standard '{standard_script_name}' not found in root of llama.cpp. Searching deeper...")
    found_scripts = !(find . -maxdepth 2 -name 'convert*.py' -print)
    if found_scripts:
        potential_paths = sorted([s.strip() for s in found_scripts], key=len)
        print(f"Found potential scripts: {potential_paths}")
        if './convert.py' in potential_paths:
            conversion_script_path_relative = './convert.py'
        elif './convert_hf_to_gguf.py' in potential_paths:
            conversion_script_path_relative = './convert_hf_to_gguf.py'
        else:
            conversion_script_path_relative = potential_paths[0]
        print(f"Using found script: {conversion_script_path_relative}")
    else:
        print("ERROR: Could not find any 'convert*.py' script.")
        conversion_script_path_relative = "./NO_SCRIPT_FOUND.py"

# Define output GGUF filenames
gguf_f16_output_name = "qwen2.5-coder-aiken-f16.gguf"
gguf_quantized_output_name = "qwen2.5-coder-aiken-q4_K_M.gguf"

# Get absolute paths for the GGUF files to be used in quantization command
# These will be relative to /content/llama.cpp where they are created
abs_gguf_f16_output_path = os.path.join(os.getcwd(), gguf_f16_output_name) # Will be /content/llama.cpp/qwen...f16.gguf
abs_gguf_quantized_output_path = os.path.join(os.getcwd(), gguf_quantized_output_name) # Will be /content/llama.cpp/qwen...q4_K_M.gguf


# Run GGUF conversion script using the venv's python
print(f"\nRunning GGUF conversion (f16) for model at: {merged_model_path_absolute}")
if os.path.exists(conversion_script_path_relative) and conversion_script_path_relative != "./NO_SCRIPT_FOUND.py":
    script_to_run = conversion_script_path_relative
    # Pass absolute path for merged model, output to current directory (llama.cpp)

    # Enhanced to cater for modell stop issue
    # conversion_command = f"{venv_python} {script_to_run} \"{merged_model_path_absolute}\" --outfile \"{gguf_f16_output_name}\" --outtype f16"

    conversion_command = f"{venv_python} {script_to_run} \"{merged_model_path_absolute}\" --outfile \"{gguf_f16_output_name}\" --outtype f16"

    print(f"Executing: {conversion_command}")
    !{conversion_command}
else:
    print(f"Skipping conversion because script was not found: {conversion_script_path_relative}")

# Check GGUF file metadata for tokenizer compatibility - recommeneded check due to sequence stopping repeat issue
if os.path.exists(abs_gguf_f16_output_path):
    print("\nChecking GGUF file metadata for tokenizer compatibility...")
    try:
        import gguf
        reader = gguf.GGUFReader(abs_gguf_f16_output_path)
        tokenizer_metadata = [field for field in reader.fields if 'tokenizer' in field.name.lower()]
        if tokenizer_metadata:
            print("Tokenizer metadata found in GGUF file.")
        else:
            print("WARNING: No tokenizer metadata found in GGUF file. Stop tokens may not work correctly.")
    except Exception as e:
        print(f"Error checking GGUF metadata: {e}")

# --- Build quantize tool using CMake ---
gguf_file_to_upload = None
gguf_local_path_in_llama_cpp = None

if os.path.exists(abs_gguf_f16_output_path): # Check the *absolute* path of the F16 GGUF
    print(f"\nSuccessfully created: {abs_gguf_f16_output_path}")
    gguf_local_path_in_llama_cpp = gguf_f16_output_name # This is still the relative name for later steps
    gguf_file_to_upload = gguf_f16_output_name # Default to f16 file for upload if quant fails

    print(f"\nAttempting quantization to Q4_K_M...")
    !rm -rf build # Clean up previous build attempts
    !rm -rf CMakeCache.txt

    print("Building quantize tool using CMake...")
    !apt-get update -qq && apt-get install -y cmake -qq > /dev/null

    !mkdir build
#     %cd build
    !cmake .. -DLLAMA_CUBLAS=OFF
    !cmake --build . --config Release

    print("\nVerifying contents of build/bin/ directory:")
    !ls -l ./bin/

    quantize_tool_path = os.path.abspath("./bin/llama-quantize") # This is the correct name

    # --- CRITICAL FIX: The faulty comment in %cd .. was removed ---
#     %cd ..

    # --- The boolean condition if quantization needed and enabled---
    quantization_enabled = True  # Set this to True or False based on your needs

    if os.path.exists(quantize_tool_path) and quantization_enabled:
        print(f"Found quantize tool at: {quantize_tool_path}")
        # Now, pass the ABSOLUTE paths for both input and output GGUF files to llama-quantize
        quantization_command = f"{quantize_tool_path} \"{abs_gguf_f16_output_path}\" \"{abs_gguf_quantized_output_path}\" q4_K_M"
        print(f"Executing: {quantization_command}")
        !{quantization_command}
        if os.path.exists(abs_gguf_quantized_output_path): # Check the absolute path for the quantized file
            print(f"Successfully created quantized model: {abs_gguf_quantized_output_path}")
            gguf_file_to_upload = gguf_quantized_output_name # Update filename for upload
            gguf_local_path_in_llama_cpp = gguf_quantized_output_name # Update filename for copy
        else:
            print("Quantization failed. Using the f16 GGUF file.")
    else:
        print(f"Could not find 'quantize' tool at {quantize_tool_path}. Using the f16 GGUF file.")
else:
    print(f"\nERROR: GGUF conversion failed or script was not run. File '{abs_gguf_f16_output_path}' not found.")

if not quantization_enabled:
  print(f"\nINFO: quantization_enabled condition has been disabled and is set to False.")

# --- Correct the final cd command ---
print(f"\nAttempting to navigate back to /content from {os.getcwd()}")
# This should now be fine, as the previous %cd .. worked
# %cd ..
print(f"Current directory after cd .. : {os.getcwd()}")

print("\nFinished GGUF conversion attempt.")

# === STEP 3.5: Test GGUF Model Inference ===
print("\n--- Testing GGUF Model Inference ---")
from transformers import pipeline, StoppingCriteriaList, StoppingCriteria

# Define custom stopping criteria
class StopOnToken(StoppingCriteria):
    def __init__(self, stop_token_id):
        self.stop_token_id = stop_token_id

    def __call__(self, input_ids, scores, **kwargs):
        return self.stop_token_id in input_ids[0][-1]

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(merged_model_path)

# Initialize pipeline with the merged PyTorch model (before GGUF conversion for simplicity)
# Note: Testing with GGUF directly in Colab is complex; here we test the merged model
pipe = pipeline("text-generation", model=merged_model_path, tokenizer=tokenizer)

# Create a test prompt
instruction_prompt = "Generate an Aiken Smart Contract for a simple Validator called Hello World."
messages = [{"role": "user", "content": instruction_prompt}]
prompt_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# Get stop token ID
stop_token_id = tokenizer.convert_tokens_to_ids("<|im_end|>")

# Generate text
print("Running test inference...")
generation_output = pipe(
    prompt_formatted,
    max_new_tokens=150,
    do_sample=True,
    temperature=0.5,
    top_k=40,
    top_p=0.9,
    eos_token_id=stop_token_id,
    stopping_criteria=StoppingCriteriaList([StopOnToken(stop_token_id)])
)

generated_text = generation_output[0]['generated_text']
assistant_response = generated_text.split("<|im_start|>assistant")[-1].strip().replace("<|im_end|>", "").strip()

print(f"\nTest Prompt:\n{prompt_formatted}")
print(f"\nTest Generated Response:\n{assistant_response}")

# === STEP 4: Create Ollama Modelfile (Corrected Syntax) ===
import os
print("\n--- Creating Ollama Modelfile ---")

# This variable should hold the name of the GGUF file created in the previous cell
# (e.g., "qwen2.5-3b-aiken-q4_K_M.gguf" or "qwen2.5-3b-aiken-f16.gguf")
# It's referenced from the variable set in Cell 4
gguf_filename_for_modelfile = gguf_file_to_upload if 'gguf_file_to_upload' in locals() and gguf_file_to_upload else None

modelfile_local_path = "Modelfile" # Standard name Ollama looks for

# Modelfile enhanced to prevent Stop sequnce errors to prevent repitition.
# added a comment to clarify their importance and ensure they are not accidentally removed.
# adjusted the inference parameters to reduce the likelihood of repetition.

if gguf_filename_for_modelfile:
    # Using f-string with triple-double-quotes as the main container
    # Using triple-single-quotes for multi-line directives inside
    modelfile_content = f"""
# Ollama Modelfile for Fine-tuned Qwen2.5-7B-Coder Aiken GGUF

# Specifies the GGUF model file to use from this repository
FROM ./{gguf_filename_for_modelfile}

# Default parameters (users can override these)
PARAMETER temperature 0.5  # Lowered to reduce randomness and potential repetition
PARAMETER top_k 40
PARAMETER top_p 0.9
# PARAMETER num_ctx 2048 # Uncomment and adjust if needed

# System message defining the assistant's role
SYSTEM '''You are a helpful assistant specialized in the Aiken smart programming language for Cardano.'''

# Prompt template matching Qwen2 Instruct format
TEMPLATE '''<|im_start|>system
{{{{ .System }}}}<|im_end|>
<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
'''

# Stop tokens crucial for Qwen models to prevent runaway generation
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
"""
    try:
        # Write the content to the local file
        with open(modelfile_local_path, "w", encoding="utf-8") as f:
            f.write(modelfile_content.strip() + "\n") # Add trailing newline, often good practice
        print(f"Modelfile created locally ('{modelfile_local_path}'), referencing GGUF: {gguf_filename_for_modelfile}")
    except Exception as e:
        print(f"Error writing Modelfile: {e}")
        modelfile_local_path = None # Ensure upload doesn't proceed if write fails
else:
    print("GGUF filename is not set (likely conversion failed). Skipping Modelfile creation.")
    modelfile_local_path = None

# === STEP 5: Verify File Existence ===
print("Verifying existence of files before copying...")
# Use the variables determined by Cell 4/5 if possible, otherwise check expected paths
gguf_to_verify = f"/content/llama.cpp/{gguf_file_to_upload}" if 'gguf_file_to_upload' in locals() and gguf_file_to_upload else "/content/llama.cpp/qwen2.5-3b-instruct-aiken-f16.gguf" # Check f16 as fallback
modelfile_to_verify = f"/content/{modelfile_local_path}" if 'modelfile_local_path' in locals() and modelfile_local_path else "/content/Modelfile"
!ls -lh {gguf_to_verify}
!ls -lh {modelfile_to_verify}

# === Last STEP 6: Copy GGUF and Modelfile to Google Drive After Huggingface API issues in Cell 6 were abandoned ===
import os
import shutil

print("Copying generated files to Google Drive...")

# --- Configuration ---
# Make sure these filenames match the ones generated in Cell 4 & 5
gguf_filename = gguf_file_to_upload if 'gguf_file_to_upload' in locals() and gguf_file_to_upload else None
modelfile_filename = modelfile_local_path if 'modelfile_local_path' in locals() and modelfile_local_path else None

# Source paths (where files were created in Colab runtime)
source_gguf_path = f"/content/llama.cpp/{gguf_filename}" if gguf_filename else None
source_modelfile_path = f"/content/{modelfile_filename}" if modelfile_filename else None

# Destination folder on Google Drive (CHOOSE WHERE YOU WANT TO SAVE THEM)
destination_folder = "/content/drive/MyDrive/qwen_aiken_gguf_for_upload"
# ---

try:
    # Ensure destination folder exists
    os.makedirs(destination_folder, exist_ok=True)

    # Copy GGUF file
    if source_gguf_path and os.path.exists(source_gguf_path):
        destination_gguf_path = os.path.join(destination_folder, gguf_filename)
        print(f"Copying {source_gguf_path} to {destination_gguf_path}...")
        shutil.copy2(source_gguf_path, destination_gguf_path)
        print("GGUF file copied.")
    else:
        print(f"ERROR: GGUF file not found at {source_gguf_path}. Cannot copy.")

    # Copy Modelfile
    if source_modelfile_path and os.path.exists(source_modelfile_path):
        destination_modelfile_path = os.path.join(destination_folder, modelfile_filename)
        print(f"Copying {source_modelfile_path} to {destination_modelfile_path}...")
        shutil.copy2(source_modelfile_path, destination_modelfile_path)
        print("Modelfile copied.")
    else:
        print(f"ERROR: Modelfile not found at {source_modelfile_path}. Cannot copy.")

except Exception as e:
    print(f"An error occurred during copy: {e}")

print("\nFile copying process finished. Check your Google Drive.")

"""Steps for Manual Upload (Assuming Files are Confirmed on Google Drive from Step above):

Phase 1: Download Files from Google Drive to Your M1 Mac

- Open Google Drive: Go to drive.google.com in your web browser and log in to the account associated with your Colab work.
- Locate Files: Navigate to the folder on your Google Drive where you saved/copied the GGUF file (e.g., qwen2.5-3b-aiken-f16.gguf) and the Modelfile.
- Download GGUF: Right-click on the .gguf file and select "Download". Wait for the download to complete (this might take a while for the ~6GB file). It will save to your Mac's default Downloads folder or wherever your browser saves files.
- Download Modelfile: Right-click on the Modelfile and select "Download". This will be very fast.
- Confirm Downloads: Check your Mac's Downloads folder (or specified location) to ensure both files are present locally.

Phase 2: Upload Files from Your M1 Mac to Hugging Face Hub

- Open Hugging Face Repo: Go to your repository page on Hugging Face (https://huggingface.co/AmiBening/Qwen2.5-3B-Aiken-GGUF) in your web browser. Make sure you are logged in as AmiBening.
- Navigate to "Files": Click on the "Files" tab of your repository.
- Initiate Upload: Click the "Add file" button and choose "Upload files" from the dropdown menu.
- Select Files from Mac: An upload interface will appear. Either:
Drag and drop both the .gguf file and the Modelfile from your Mac's Downloads folder into the designated area on the webpage.
Or, click to browse and navigate to your Downloads folder, selecting both files (you can usually select multiple files using Cmd+Click).
- Add Commit Message: In the text box at the bottom, type a brief description, for example: Upload final GGUF model and Modelfile.
- Commit Changes: Ensure the commit target is the main branch (usually the default) and click the "Commit changes" button.
- Wait for Upload: The upload process will begin. The Modelfile will upload instantly, but the large .gguf file will take some time depending on your internet connection speed. Keep the browser tab open until the upload completes. You should see progress indicators.
- Verify: Once finished, refresh the "Files" tab on your Hugging Face repository page. You should see both qwen2.5-3b-aiken-f16.gguf and Modelfile listed.

Now the files are on the Hugging Face Hub, and users (including yourself) should be able to use ollama run AmiBening/Qwen2.5-3B-Aiken-GGUF on their local machines.
"""

# Go onto Huggingface and create a new model with the name Qwen2.5-3B-Aiken-GGUF as below , the API would not allow me to create it

# === STEP 7: Upload to Hugging Face Hub (Use Google Drive source as that is where files are copied to in Step 6) ===
print("\n--- Uploading files to Hugging Face Hub from Google Drive ---")

from huggingface_hub import HfApi, create_repo
import os
from google.colab import userdata # Import userdata to access secrets
from google.colab import drive # Import drive to ensure it's mounted

# --- Configuration for Hugging Face Upload ---
# Get HF_TOKEN securely from Colab Secrets
try:
    HF_TOKEN = userdata.get('HF_TOKEN')
    if not HF_TOKEN:
        raise ValueError("HF_TOKEN secret not found or empty. Please set it in Colab's 'Secrets' tab.")
    print("Hugging Face token successfully loaded from Colab Secrets.")
except Exception as e:
    print(f"Error loading Hugging Face token from secrets: {e}")
    HF_TOKEN = None # Set to None if there's an issue, preventing upload

HF_REPO_ID = "AmiBening/Qwen2.5-Coder-Aiken-GGUF" # Format: "your_username/your_repo_name"
# Ensure the repository is PUBLIC if you want others to access it easily

# --- Ensure Google Drive is mounted ---
try:
    if not os.path.exists("/content/drive/MyDrive"):
        print("Mounting Google Drive...")
        drive.mount('/content/drive')
    else:
        print("Google Drive already mounted.")
except Exception as e:
    print(f"Could not mount Google Drive: {e}")
    # Decide if you want to exit or continue with error if drive fails to mount
    # For now, we'll just print an error and the upload will likely fail due to missing files.


# --- Determine Absolute Paths for Upload from Google Drive ---
# These variables should ideally be set correctly by Step 6's successful execution.
# We will explicitly use the `destination_folder` from Step 6.
destination_folder = "/content/drive/MyDrive/qwen_aiken_gguf_for_upload" # This must match Step 6's output folder

gguf_filename = globals().get('gguf_file_to_upload') # Get filename from previous steps
modelfile_filename = globals().get('modelfile_local_path') # Get filename from previous steps

# Add a robust check for the filenames, as they are crucial
if not gguf_filename:
    print("WARNING: 'gguf_file_to_upload' was not set. Attempting to infer default GGUF filename.")
    # Fallback to expected filenames if the variable isn't propagated
    if os.path.exists(os.path.join(destination_folder, "qwen2.5-coder-aiken-q4_K_M.gguf")):
        gguf_filename = "qwen2.5-coder-aiken-q4_K_M.gguf"
    elif os.path.exists(os.path.join(destination_folder, "qwen2.5-coder-aiken-f16.gguf")):
        gguf_filename = "qwen2.5-coder-aiken-f16.gguf"

if not modelfile_filename:
    print("WARNING: 'modelfile_local_path' was not set. Attempting to infer default Modelfile filename.")
    # Fallback to expected Modelfile name
    if os.path.exists(os.path.join(destination_folder, "Modelfile")):
        modelfile_filename = "Modelfile"

# Construct the full paths to the files in Google Drive
path_to_gguf_file = os.path.join(destination_folder, gguf_filename) if gguf_filename else None
path_to_modelfile = os.path.join(destination_folder, modelfile_filename) if modelfile_filename else None

print(f"Attempting to upload GGUF file from Google Drive: {path_to_gguf_file}")
print(f"Attempting to upload Modelfile from Google Drive: {path_to_modelfile}")

# --- Main Upload Logic ---
if HF_TOKEN:
    # Check if paths are valid before proceeding
    if not path_to_gguf_file or not path_to_modelfile:
        print("ERROR: One or both file paths (GGUF or Modelfile) are not valid. Cannot proceed with Hugging Face upload.")
    elif not os.path.exists(path_to_gguf_file):
        print(f"ERROR: GGUF file not found in Google Drive at {path_to_gguf_file}. Please ensure Step 6 completed successfully.")
    elif not os.path.exists(path_to_modelfile):
        print(f"ERROR: Modelfile not found in Google Drive at {path_to_modelfile}. Please ensure Step 6 completed successfully.")
    else:
        try:
            api = HfApi(token=HF_TOKEN)

            print(f"Attempting to create repository: {HF_REPO_ID} (if it doesn't exist)...")
            create_repo(repo_id=HF_REPO_ID, token=HF_TOKEN, private=False, exist_ok=True)
            print(f"Repository {HF_REPO_ID} ensured.")

            # Upload GGUF file
            print(f"Uploading {gguf_filename} to {HF_REPO_ID}...")
            api.upload_file(
                path_or_fileobj=path_to_gguf_file,
                path_in_repo=gguf_filename, # Keep original filename in repo
                repo_id=HF_REPO_ID,
                repo_type="model",
                commit_message=f"Upload {gguf_filename} from Google Drive"
            )
            print(f"Successfully uploaded {gguf_filename}.")

            # Upload Modelfile
            print(f"Uploading {modelfile_filename} to {HF_REPO_ID}...")
            api.upload_file(
                path_or_fileobj=path_to_modelfile,
                path_in_repo="Modelfile", # Always name it Modelfile in the repo root
                repo_id=HF_REPO_ID,
                repo_type="model",
                commit_message=f"Upload Modelfile for {gguf_filename} from Google Drive"
            )
            print(f"Successfully uploaded {modelfile_filename}.")

            print(f"\nUpload process completed. Check your Hugging Face repository: https://huggingface.co/{HF_REPO_ID}/tree/main")

        except Exception as e:
            print(f"An error occurred during Hugging Face upload: {e}")
else:
    print("Hugging Face upload skipped due to missing or invalid token.")